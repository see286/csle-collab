import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import pickle
import json
import os
import matplotlib.pyplot as plt

# ==================== SimplePOMDP ç±»å®šä¹‰ ====================
class SimplePOMDP:
    """ç®€åŒ–çš„POMDPç±»ï¼ˆä¸build_pomdp.pyä¸­ç›¸åŒï¼‰"""
    def __init__(self, transition_model, observation_model, reward_model):
        self.transition_model = transition_model
        self.observation_model = observation_model
        self.reward_model = reward_model
        
        self.n_states = transition_model.shape[0]
        self.n_actions = transition_model.shape[1]
        self.n_observations = observation_model.shape[1]
        
        # åˆå§‹ä¿¡å¿µçŠ¶æ€ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        self.belief = np.ones(self.n_states) / self.n_states
    
    def update_belief(self, action, observation):
        """æ›´æ–°ä¿¡å¿µçŠ¶æ€ï¼ˆè´å¶æ–¯æ›´æ–°ï¼‰"""
        new_belief = np.zeros(self.n_states)
        
        for s_next in range(self.n_states):
            # P(s' | o, a, b) âˆ P(o | s') * Î£_s P(s' | s, a) * b(s)
            prob_o_given_s = self.observation_model[s_next, observation]
            sum_term = 0
            
            for s in range(self.n_states):
                sum_term += self.transition_model[s, action, s_next] * self.belief[s]
            
            new_belief[s_next] = prob_o_given_s * sum_term
        
        # å½’ä¸€åŒ–
        if new_belief.sum() > 0:
            new_belief /= new_belief.sum()
        else:
            new_belief = np.ones(self.n_states) / self.n_states
        
        self.belief = new_belief
        return self.belief
    
    def get_action_probabilities(self, belief=None):
        """è·å–æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼ˆç®€å•å¯å‘å¼ï¼‰"""
        if belief is None:
            belief = self.belief
        
        # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„æœŸæœ›å¥–åŠ±
        expected_rewards = np.zeros(self.n_actions)
        
        for a in range(self.n_actions):
            for s in range(self.n_states):
                expected_rewards[a] += belief[s] * self.reward_model[s, a]
        
        # ä½¿ç”¨softmaxè½¬æ¢ä¸ºæ¦‚ç‡
        exp_rewards = np.exp(expected_rewards - np.max(expected_rewards))
        action_probs = exp_rewards / exp_rewards.sum()
        
        return action_probs
    
    def reset_belief(self):
        """é‡ç½®ä¿¡å¿µçŠ¶æ€"""
        self.belief = np.ones(self.n_states) / self.n_states
# ==================== SimplePOMDP ç±»å®šä¹‰ç»“æŸ ====================

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)

class PPOActorCritic(nn.Module):
    """PPO Actor-Criticç½‘ç»œ"""
    def __init__(self, n_states, n_actions, hidden_dim=64):
        super(PPOActorCritic, self).__init__()
        
        # å…±äº«çš„ç‰¹å¾æå–å±‚
        self.shared = nn.Sequential(
            nn.Linear(n_states, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )
        
        # ç­–ç•¥ç½‘ç»œ (Actor)
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, n_actions),
            nn.Softmax(dim=-1)
        )
        
        # ä»·å€¼ç½‘ç»œ (Critic)
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        features = self.shared(state)
        action_probs = self.actor(features)
        state_value = self.critic(features)
        return action_probs, state_value

class PPOTrainer:
    """PPOè®­ç»ƒå™¨"""
    def __init__(self, pomdp, n_states, n_actions):
        self.pomdp = pomdp
        self.n_states = n_states
        self.n_actions = n_actions
        
        # åˆå§‹åŒ–PPOç½‘ç»œ
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.policy = PPOActorCritic(n_states, n_actions).to(self.device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=3e-4)
        
        # PPOè¶…å‚æ•°
        self.gamma = 0.99
        self.clip_epsilon = 0.2
        self.update_epochs = 10
        self.batch_size = 64
        
        # è®­ç»ƒè®°å½•
        self.rewards_history = []
        self.episode_lengths = []
    
    def select_action(self, belief):
        """æ ¹æ®å½“å‰ä¿¡å¿µé€‰æ‹©åŠ¨ä½œ"""
        belief_tensor = torch.FloatTensor(belief).to(self.device)
        
        with torch.no_grad():
            action_probs, state_value = self.policy(belief_tensor.unsqueeze(0))
        
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        
        return action.item(), log_prob, state_value
    
    def compute_returns(self, rewards, values, dones, next_value):
        """è®¡ç®—GAEå›æŠ¥"""
        returns = []
        gae = 0
        
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * 0.95 * (1 - dones[t]) * gae
            returns.insert(0, gae + values[t])
            next_value = values[t]
        
        return returns
    
    def update_policy(self, states, actions, log_probs_old, returns, advantages):
        """æ›´æ–°ç­–ç•¥ç½‘ç»œ"""
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        log_probs_old = torch.FloatTensor(log_probs_old).to(self.device)
        returns = torch.FloatTensor(returns).to(self.device)
        advantages = torch.FloatTensor(advantages).to(self.device)
        
        # å½’ä¸€åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # å¤šè½®æ›´æ–°
        for _ in range(self.update_epochs):
            action_probs, values = self.policy(states)
            dist = Categorical(action_probs)
            log_probs_new = dist.log_prob(actions)
            
            # æ¦‚ç‡æ¯”ç‡
            ratios = torch.exp(log_probs_new - log_probs_old)
            
            # è£å‰ªç›®æ ‡å‡½æ•°
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            
            # ç­–ç•¥æŸå¤±
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ä»·å€¼æŸå¤±
            value_loss = nn.MSELoss()(values.squeeze(), returns)
            
            # æ€»æŸå¤±
            loss = policy_loss + 0.5 * value_loss
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
            self.optimizer.step()
    
    def train(self, num_episodes=100, max_steps=30):
        """è®­ç»ƒPPOä»£ç†"""
        print(f"ğŸš€ å¼€å§‹PPOè®­ç»ƒï¼Œå…±{num_episodes}å›åˆ")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   çŠ¶æ€æ•°: {self.n_states}, åŠ¨ä½œæ•°: {self.n_actions}")
        
        for episode in range(num_episodes):
            # é‡ç½®ç¯å¢ƒ
            self.pomdp.reset_belief()
            episode_reward = 0
            episode_steps = 0
            
            # å­˜å‚¨è½¨è¿¹
            states = []
            actions = []
            log_probs = []
            values = []
            rewards = []
            dones = []
            
            for step in range(max_steps):
                # è·å–å½“å‰ä¿¡å¿µ
                current_belief = self.pomdp.belief
                
                # é€‰æ‹©åŠ¨ä½œ
                action, log_prob, value = self.select_action(current_belief)
                
                # æ¨¡æ‹Ÿç¯å¢ƒæ­¥éª¤ï¼ˆä½¿ç”¨POMDPçš„å¥–åŠ±æ¨¡å‹ï¼‰
                state_idx = np.argmax(current_belief) if current_belief.max() > 0.5 else 0
                reward = self.pomdp.reward_model[state_idx, action] + np.random.normal(0, 0.1)
                
                # éšæœºç”Ÿæˆè§‚æµ‹ï¼ˆç®€åŒ–ï¼‰
                observation = np.random.randint(0, self.pomdp.n_observations)
                
                # æ›´æ–°ä¿¡å¿µ
                self.pomdp.update_belief(action, observation)
                
                # å­˜å‚¨è½¨è¿¹æ•°æ®
                states.append(current_belief)
                actions.append(action)
                log_probs.append(log_prob.item())
                values.append(value.item())
                rewards.append(reward)
                dones.append(0 if step < max_steps-1 else 1)
                
                episode_reward += reward
                episode_steps += 1
            
            # è®¡ç®—æœ€åä¸€ä¸ªçŠ¶æ€çš„value
            final_belief = self.pomdp.belief
            final_belief_tensor = torch.FloatTensor(final_belief).to(self.device)
            with torch.no_grad():
                _, final_value = self.policy(final_belief_tensor.unsqueeze(0))
            
            # è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
            returns = self.compute_returns(rewards, values, dones, final_value.item())
            advantages = np.array(returns) - np.array(values)
            
            # æ›´æ–°ç­–ç•¥
            self.update_policy(states, actions, log_probs, returns, advantages)
            
            # è®°å½•è®­ç»ƒè¿›åº¦
            self.rewards_history.append(episode_reward)
            self.episode_lengths.append(episode_steps)
            
            # æ‰“å°è¿›åº¦
            if (episode + 1) % 20 == 0:
                avg_reward = np.mean(self.rewards_history[-20:])
                print(f"å›åˆ {episode+1}/{num_episodes} | "
                      f"å¹³å‡å¥–åŠ±: {avg_reward:.3f} | "
                      f"å›åˆæ­¥æ•°: {episode_steps}")
        
        print("âœ… PPOè®­ç»ƒå®Œæˆ!")
        return self.policy
    
    def save_model(self, path):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        torch.save({
            'policy_state_dict': self.policy.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'n_states': self.n_states,
            'n_actions': self.n_actions,
            'rewards_history': self.rewards_history,
            'episode_lengths': self.episode_lengths
        }, path)
        print(f"âœ… PPOæ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    def plot_training(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(12, 4))
        
        # å¥–åŠ±æ›²çº¿
        plt.subplot(1, 2, 1)
        plt.plot(self.rewards_history)
        plt.xlabel('Episode')
        plt.ylabel('Total Reward')
        plt.title('PPO Training Reward History')
        plt.grid(True)
        
        # æ»‘åŠ¨å¹³å‡å¥–åŠ±
        window_size = 20
        if len(self.rewards_history) >= window_size:
            moving_avg = np.convolve(self.rewards_history, np.ones(window_size)/window_size, mode='valid')
            plt.plot(range(window_size-1, len(self.rewards_history)), moving_avg, 'r-', linewidth=2, label=f'{window_size}-ep moving avg')
            plt.legend()
        
        # å›åˆé•¿åº¦
        plt.subplot(1, 2, 2)
        plt.plot(self.episode_lengths)
        plt.xlabel('Episode')
        plt.ylabel('Episode Length')
        plt.title('Episode Length History')
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('/home/li/csle/ppo_training_curves.png', dpi=100)
        print("âœ… è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜")
        plt.close()  # ä¸æ˜¾ç¤ºå›¾å½¢ï¼Œåªä¿å­˜æ–‡ä»¶

def main():
    print("ğŸ¯ Day 5: PPOè®­ç»ƒå¼€å§‹")
    
    # åŠ è½½POMDPæ¨¡å‹ - ç°åœ¨ä½¿ç”¨æ–°çš„åŠ è½½æ–¹å¼
    model_file = "/home/li/csle/estimated_models.pkl"
    pomdp_file = "/home/li/csle/pomdp_object.pkl"
    
    if not os.path.exists(model_file):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")
        print("è¯·å…ˆå®ŒæˆDay 3ä»»åŠ¡")
        return
    
    if not os.path.exists(pomdp_file):
        print(f"âŒ POMDPæ–‡ä»¶ä¸å­˜åœ¨: {pomdp_file}")
        print("å°è¯•é‡æ–°æ„å»ºPOMDP...")
        # å¯ä»¥åœ¨è¿™é‡Œé‡æ–°æ„å»ºï¼Œä½†ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ç›´æ¥åŠ è½½åŸå§‹æ¨¡å‹
        pass
    
    try:
        # æ–¹æ³•1ï¼šå°è¯•åŠ è½½pickleæ–‡ä»¶
        if os.path.exists(pomdp_file):
            try:
                with open(pomdp_file, 'rb') as f:
                    pomdp = pickle.load(f)
                print("âœ… æˆåŠŸåŠ è½½POMDPå¯¹è±¡")
            except:
                print("âš ï¸  pickleåŠ è½½å¤±è´¥ï¼Œé‡æ–°æ„å»ºPOMDP...")
                # åŠ è½½åŸå§‹æ¨¡å‹å¹¶é‡æ–°æ„å»ºPOMDP
                with open(model_file, 'rb') as f:
                    models = pickle.load(f)
                
                pomdp = SimplePOMDP(
                    models['transition_model'],
                    models['observation_model'],
                    models['reward_model']
                )
                print("âœ… é‡æ–°æ„å»ºPOMDPæˆåŠŸ")
        else:
            # ç›´æ¥åŠ è½½åŸå§‹æ¨¡å‹å¹¶æ„å»ºPOMDP
            with open(model_file, 'rb') as f:
                models = pickle.load(f)
            
            pomdp = SimplePOMDP(
                models['transition_model'],
                models['observation_model'],
                models['reward_model']
            )
            print("âœ… ä»åŸå§‹æ¨¡å‹æ„å»ºPOMDPæˆåŠŸ")
        
        print(f"   - çŠ¶æ€æ•°: {pomdp.n_states}")
        print(f"   - åŠ¨ä½œæ•°: {pomdp.n_actions}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = PPOTrainer(pomdp, pomdp.n_states, pomdp.n_actions)
        
        # å¼€å§‹è®­ç»ƒï¼ˆç®€åŒ–ä¸º80å›åˆï¼ŒåŠ å¿«é€Ÿåº¦ï¼‰
        print("\nå¼€å§‹è®­ç»ƒPPOç­–ç•¥...")
        policy = trainer.train(num_episodes=80, max_steps=25)
        
        # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        trainer.save_model('/home/li/csle/ppo_policy.pth')
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        trainer.plot_training()
        
        # æµ‹è¯•è®­ç»ƒåçš„ç­–ç•¥
        print("\nğŸ§ª æµ‹è¯•è®­ç»ƒåçš„ç­–ç•¥:")
        pomdp.reset_belief()
        test_belief = pomdp.belief
        
        # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        belief_tensor = torch.FloatTensor(test_belief).to(trainer.device)
        with torch.no_grad():
            action_probs, _ = trainer.policy(belief_tensor.unsqueeze(0))
        
        print(f"åˆå§‹ä¿¡å¿µ: {test_belief}")
        action_probs_np = action_probs.squeeze().cpu().numpy()
        print(f"åŠ¨ä½œæ¦‚ç‡: {action_probs_np}")
        print(f"æ¨èåŠ¨ä½œ: {np.argmax(action_probs_np)}")
        
        # ç”Ÿæˆç»™Cçš„æ›´æ–°æ–‡ä»¶
        output_for_C = {
            'trained_policy': {
                'file': '/home/li/csle/ppo_policy.pth',
                'format': 'PyTorch state_dict',
                'description': 'PPOè®­ç»ƒåçš„ç­–ç•¥ç½‘ç»œ'
            },
            'training_results': {
                'final_avg_reward': float(np.mean(trainer.rewards_history[-20:])),
                'total_episodes': len(trainer.rewards_history),
                'avg_episode_length': float(np.mean(trainer.episode_lengths)),
                'training_curves': '/home/li/csle/ppo_training_curves.png'
            },
            'model_architecture': {
                'n_states': pomdp.n_states,
                'n_actions': pomdp.n_actions,
                'hidden_dim': 64
            },
            'test_results': {
                'initial_belief': test_belief.tolist(),
                'action_probabilities': action_probs_np.tolist(),
                'recommended_action': int(np.argmax(action_probs_np))
            },
            'day': 5,
            'task': 'PPOè®­ç»ƒå®Œæˆ'
        }
        
        output_file = '/home/li/csle/ppo_training_results.json'
        with open(output_file, 'w') as f:
            json.dump(output_for_C, f, indent=2)
        
        print(f"\nâœ… PPOè®­ç»ƒå®Œæˆï¼Œäº§å‡ºæ–‡ä»¶:")
        print(f"   1. ç­–ç•¥æ¨¡å‹: /home/li/csle/ppo_policy.pth")
        print(f"   2. è®­ç»ƒæ›²çº¿: /home/li/csle/ppo_training_curves.png")
        print(f"   3. è®­ç»ƒç»“æœ: {output_file}")
        print(f"   4. æœ€å20å›åˆå¹³å‡å¥–åŠ±: {np.mean(trainer.rewards_history[-20:]):.3f}")
        
        print("\nğŸ¯ Day 5 ä»»åŠ¡å®Œæˆï¼å‡†å¤‡è¿›å…¥Day 6: ç­–ç•¥è°ƒä¼˜")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
